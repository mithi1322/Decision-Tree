{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2720b9e-02b5-40de-9c87-e1cb938a235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. \n",
    "    Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or \n",
    "    random search to find the best hyperparameters.\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45966fc-432b-42fb-be93-1d110f900758",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "# Answer 1:\n",
    "Gradient Boosting Regression is a machine learning algorithm that belongs to the family of ensemble methods and is used for both regression and classification tasks. It is an extension of the gradient boosting algorithm that aims to minimize the errors in prediction by sequentially adding weak learners (usually decision trees) to the model.\n",
    "\n",
    "In Gradient Boosting Regression, the algorithm starts with an initial prediction, often the mean of the target variable for regression problems. Then, in each iteration, a new weak learner (e.g., decision tree) is added to the model to correct the errors made by the previous weak learners. The new weak learner is trained on the negative gradient (residuals) of the loss function with respect to the previous prediction. The learning process continues iteratively until a predefined number of weak learners (trees) are added, or until a certain level of accuracy is achieved.\n",
    "\n",
    "The final prediction is the weighted sum of all weak learners' predictions, where the weights are determined based on their individual performance during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d9f5f-78ab-44bb-a433-724927015a75",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "# Answer 2:\n",
    "To implement a simple gradient boosting algorithm from scratch in Python and NumPy, follow these steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1cdd2f6-f836-45d4-8234-48da7ebc5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the base learner - Decision Tree\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y, depth=0)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        # Base case: If only one sample or reached max_depth, return the average value of y\n",
    "        if num_samples == 1 or depth == self.max_depth:\n",
    "            return np.mean(y)\n",
    "\n",
    "        # Find the best feature and split point that minimize the squared error\n",
    "        best_feature, best_split = None, None\n",
    "        min_error = float('inf')\n",
    "        for feature in range(num_features):\n",
    "            for split in np.unique(X[:, feature]):\n",
    "                y_left = y[X[:, feature] < split]\n",
    "                y_right = y[X[:, feature] >= split]\n",
    "                error = np.sum((y_left - np.mean(y_left))**2) + np.sum((y_right - np.mean(y_right))**2)\n",
    "                if error < min_error:\n",
    "                    best_feature, best_split = feature, split\n",
    "                    min_error = error\n",
    "\n",
    "        # Split the data and recursively grow the left and right subtrees\n",
    "        left_indices = X[:, best_feature] < best_split\n",
    "        right_indices = X[:, best_feature] >= best_split\n",
    "        left_tree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return (best_feature, best_split, left_tree, right_tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_tree(self, x, tree):\n",
    "        if not isinstance(tree, tuple):\n",
    "            return tree\n",
    "        feature, split, left_subtree, right_subtree = tree\n",
    "        subtree = left_subtree if x[feature] < split else right_subtree\n",
    "        return self._predict_tree(x, subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26c900b-1cf8-4877-849d-9ec873ae825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient boosting algorithm\n",
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the model with the mean of y\n",
    "        initial_prediction = np.mean(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - initial_prediction\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.estimators.append(tree)\n",
    "            initial_prediction += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.estimators])\n",
    "        return np.sum(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e855a066-6965-4307-a565-b9d369421653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 683.961752150008\n",
      "R-squared: -84.495219018751\n"
     ]
    }
   ],
   "source": [
    "# Create a simple dataset for regression\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb = GradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X, y)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = gb.predict(X)\n",
    "\n",
    "# Calculate Mean Squared Error and R-squared\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "mean_y = np.mean(y)\n",
    "r_squared = 1 - mse / np.mean((y - mean_y)**2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b87cb-b7b7-4246-9458-0bde987dd94b",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "# Answer 3:\n",
    "To experiment with different hyperparameters, you can use grid search or random search to find the best combination of hyperparameters that optimizes the performance of the gradient boosting model. Grid search exhaustively tries all possible combinations of hyperparameters, while random search samples hyperparameters randomly.\n",
    "\n",
    "For example, using scikit-learn's GridSearchCV for grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e767b57-fe63-419d-bee0-05ff3aa35d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best Mean Squared Error: nan\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and performance\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Mean Squared Error:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474be87-c1dc-46aa-9f15-18e4d2795639",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "# Answer 4:\n",
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing or a random baseline. In the context of regression problems, weak learners are typically shallow decision trees (also known as decision stumps) with limited depth, one split, or a small number of splits. For classification problems, weak learners can be simple decision trees with low depth or even simple linear models.\n",
    "\n",
    "The key characteristic of a weak learner is that it is not highly expressive and does not capture complex patterns in the data. However, when combined in an ensemble through the boosting process, weak learners can be progressively improved and contribute to the construction of a strong learner that achieves higher predictive performance.\n",
    "\n",
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "# Answer 5:\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively correct the errors made by the previous weak learners to improve overall predictive performance. The algorithm starts with an initial prediction, often the mean of the target variable for regression problems, and then adds weak learners to the model to reduce the prediction error.\n",
    "\n",
    "At each iteration, a new weak learner is trained to focus on the residuals (negative gradients) of the loss function with respect to the current predictions. This allows the weak learner to learn the patterns or relationships missed by the previous weak learners. The predictions of all weak learners are combined to obtain the final prediction, with more emphasis on the predictions of the better-performing weak learners.\n",
    "\n",
    "The boosting process continues until a predefined number of weak learners are added, or until a certain level of accuracy is achieved. The sequential nature of the algorithm and the focus on correcting errors make Gradient Boosting an effective ensemble learning technique.\n",
    "\n",
    "# Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "# Answer 6:\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and iterative manner. The process can be summarized as follows:\n",
    "\n",
    "Initialize the predictions as the mean of the target variable for regression or class probabilities for classification.\n",
    "\n",
    "Calculate the negative gradient (residuals) of the loss function with respect to the current predictions.\n",
    "\n",
    "Train a new weak learner (e.g., decision tree) to fit the negative gradients.\n",
    "\n",
    "Update the predictions by adding the weighted predictions of the new weak learner to the current predictions.\n",
    "\n",
    "Repeat steps 2 to 4 for a specified number of iterations or until a certain level of accuracy is achieved.\n",
    "\n",
    "The key idea is that each new weak learner is trained to correct the errors made by the previous weak learners, gradually reducing the overall prediction error. The predictions of all weak learners are combined using weighted averaging for regression or weighted voting for classification, with more accurate weak learners having higher weights.\n",
    "\n",
    "# Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?\n",
    "# Answer 7:\n",
    "The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "Define the loss function: The first step is to define a loss function that measures the error between the actual target values and the predicted values. For regression problems, the mean squared error (MSE) or the mean absolute error (MAE) is commonly used as the loss function. For classification problems, the cross-entropy loss is typically used.\n",
    "\n",
    "Initialize the predictions: In the first iteration, the predictions are initialized with a simple model, often the mean of the target variable for regression or the class probabilities for classification.\n",
    "\n",
    "Calculate the negative gradient (residuals): The negative gradient of the loss function with respect to the current predictions is calculated. This represents the direction in which the predictions need to be corrected to minimize the loss.\n",
    "\n",
    "Train a new weak learner: A new weak learner, such as a decision tree, is trained to fit the negative gradients. The weak learner is typically shallow with limited depth to avoid overfitting.\n",
    "\n",
    "Update the predictions: The predictions are updated by adding the weighted predictions of the new weak learner to the current predictions. The weight assigned to the new weak learner is determined based on its performance and the learning rate, which controls the step size in the optimization process.\n",
    "\n",
    "Repeat the process: Steps 3 to 5 are repeated for a specified number of iterations or until a stopping criterion is met. Each new weak learner focuses on correcting the errors made by the previous weak learners, leading to gradual improvement in predictive performance.\n",
    "\n",
    "Final prediction: The final prediction is obtained by combining the predictions of all weak learners, with more accurate weak learners having higher weights in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bf0b3-94e4-458b-91da-984cf9866959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
