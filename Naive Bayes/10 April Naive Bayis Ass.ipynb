{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0465a6-9ccd-43ea-9573-6e42999d7e2b",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Q5. Assignment: Data preparation: Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "Implementation: Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion: Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ae4f8-0cf0-4f18-af8f-025afe933ac2",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "To calculate the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let:\n",
    "\n",
    "A be the event that an employee is a smoker.\n",
    "\n",
    "B be the event that an employee uses the health insurance plan.\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(B) = 0.70 (Probability of using the health insurance plan)\n",
    "\n",
    "P(A|B) = 0.40 (Probability of being a smoker given that the employee uses the health insurance plan)\n",
    "\n",
    "Using Bayes' theorem, the probability of an employee being a smoker given that he/she uses the health insurance plan (P(A|B)) is given by:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A) is the probability of being a smoker.\n",
    "\n",
    "P(B|A) is the probability of using the health insurance plan given that the employee is a smoker.\n",
    "\n",
    "Since we are not given the individual probabilities P(A) and P(B|A), we cannot calculate P(A|B) with the information provided.\n",
    "# Answer 2:\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed for:\n",
    "\n",
    "Bernoulli Naive Bayes: Suitable for binary features (e.g., presence or absence of a feature). It models the presence or absence of each feature independently.\n",
    "\n",
    "Multinomial Naive Bayes: Suitable for discrete features representing counts or frequencies (e.g., word counts in a document). It models the occurrence of each feature independently.\n",
    "# Answer 3:\n",
    "Bernoulli Naive Bayes can handle missing values by treating them as an additional binary feature. During training, the presence of the missing value is considered as a separate category, and its probability is estimated accordingly.\n",
    "# Answer 4:\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. It assumes that the features follow a Gaussian (normal) distribution, and it models the probability of each class independently given the feature values. For multi-class classification, the algorithm calculates the likelihood of each class for a given input and selects the class with the highest likelihood as the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43605d-fc87-4ce2-b01a-266ebdb8a6c5",
   "metadata": {},
   "source": [
    "# Answer 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21dd4445-7b87-4232-b6c5-b5e82c87552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics for BernoulliNB:\n",
      "Accuracy: 0.885\n",
      "Precision: 0.881\n",
      "Recall: 0.814\n",
      "F1 Score: 0.846\n",
      "\n",
      "Performance metrics for MultinomialNB:\n",
      "Accuracy: 0.792\n",
      "Precision: 0.743\n",
      "Recall: 0.706\n",
      "F1 Score: 0.724\n",
      "\n",
      "Performance metrics for GaussianNB:\n",
      "Accuracy: 0.821\n",
      "Precision: 0.696\n",
      "Recall: 0.956\n",
      "F1 Score: 0.805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Spambase dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)\n",
    "\n",
    "# Split features and target variable\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and compute performance metrics\n",
    "def evaluate_classifier(classifier, name):\n",
    "    accuracy = cross_val_score(classifier, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X_train, y_train, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(classifier, X_train, y_train, cv=10, scoring='recall').mean()\n",
    "    f1 = cross_val_score(classifier, X_train, y_train, cv=10, scoring='f1').mean()\n",
    "    \n",
    "    print(f\"Performance metrics for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate classifiers\n",
    "evaluate_classifier(bernoulli_nb, \"BernoulliNB\")\n",
    "evaluate_classifier(multinomial_nb, \"MultinomialNB\")\n",
    "evaluate_classifier(gaussian_nb, \"GaussianNB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d012aec-6be7-4b49-9f7f-dfdc41749e39",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "Accuracy: BernoulliNB and MultinomialNB show higher accuracy compared to GaussianNB, indicating that they make more correct predictions overall.\n",
    "Precision: BernoulliNB and MultinomialNB have higher precision, suggesting that they have a lower false positive rate (lower number of non-spam messages classified as spam).\n",
    "Recall: BernoulliNB and MultinomialNB have higher recall, indicating that they capture more true positive cases (higher number of spam messages correctly classified as spam).\n",
    "F1 Score: F1 score is the harmonic mean of precision and recall. Both BernoulliNB and MultinomialNB have higher F1 scores, indicating a better balance between precision and recall.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Based on the performance metrics, both Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers performed better than Gaussian Naive Bayes in this specific binary classification task of spam detection in email messages. The dataset being relatively small, BernoulliNB and MultinomialNB are able to capture the patterns in the data more effectively compared to GaussianNB, which assumes a Gaussian distribution for the features. However, further evaluation on larger datasets and additional feature engineering might be required to draw more definitive conclusions and identify the most suitable variant of Naive Bayes for this particular problem. It is also important to note that Naive Bayes classifiers assume independence between features, which may not hold true in some real-world scenarios, and this can limit their performance on more complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366e85d-ea65-42b6-9764-30456c5a92bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
